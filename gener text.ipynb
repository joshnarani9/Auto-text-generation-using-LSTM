{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1563951906596,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "Q7QeYP-5wiWz",
    "outputId": "63522b74-6caa-4ff3-ca96-ee9c62f25055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6553,
     "status": "ok",
     "timestamp": 1563859450106,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "mWPWglyozGkK",
    "outputId": "28352540-6ac0-4a8e-96f3-23bf8296fdd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aadhaarr.jpeg\n",
      " adcourse.rar\n",
      "'Advantages and disadvantages of.pptx'\n",
      " AI\n",
      "'AI and Deep Learning using TensorFLow'\n",
      "'Anyfile Notepad Files'\n",
      " assignments\n",
      " BelgiumTSC_Testing.zip\n",
      " BelgiumTSC_Training.zip\n",
      " business.txt.gdoc\n",
      " cifar\n",
      "'Colab Notebooks'\n",
      " creditcard.csv\n",
      " customers.ipynb\n",
      "'diabetes identification using knn.rar'\n",
      " Documentation-JR.gdoc\n",
      " dog_cat_small_data.zip\n",
      " IMG_20150307_202933143.jpg\n",
      " joan.jpg\n",
      " jo.docx\n",
      " joshi.jpg\n",
      "'joshna rani ANN  - joshna rani Pothuganti.pdf'\n",
      "'joshna rani pothuganti_Artificial neural networks.ipynb'\n",
      "'joshna rani pothuganti_Artificial neural networks - joshna rani Pothuganti.ipynb'\n",
      " notebook\n",
      "'opencv and yolo'\n",
      "'pan card.jpg'\n",
      "'pass photo.docx'\n",
      "'profile pic.jpg'\n",
      " recominp.py\n",
      " SampleTextFile.txt\n",
      " Screenshot_2016-02-11-21-34-11-027.jpeg\n",
      " TCOE\n",
      "'TEAM REGEX Meeting.gdoc'\n",
      " test.csv\n",
      " text.ipynb\n",
      " Text.txt\n",
      " train.csv\n",
      "'Untitled document.gdoc'\n",
      " v.pdf\n",
      " WIN_20141002_180434.JPG\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3429,
     "status": "ok",
     "timestamp": 1563951917104,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "SMElWNx5wTSM",
    "outputId": "066aa7d3-960f-4ffa-aee9-5558d0c2480f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2050,
     "status": "ok",
     "timestamp": 1563951922544,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "dBT-tNR-LIxk",
    "outputId": "ebc6f001-4a94-47e7-9a8f-b58fbb492dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please take a look at the important information in this header.\n",
      "We encourage you to keep this file on your own disk, keeping an\n",
      "electronic path open for the next readers.  Do not remove this.\n",
      "\n",
      "\n",
      "**Welc\n",
      "['please', 'take', 'a', 'look', 'at', 'the', 'important', 'information', 'in', 'this', 'header', 'we', 'encourage', 'you', 'to', 'keep', 'this', 'file', 'on', 'your', 'own', 'disk', 'keeping', 'an', 'electronic', 'path', 'open', 'for', 'the', 'next', 'readers', 'do', 'not', 'remove', 'this', 'welcome', 'to', 'the', 'world', 'of', 'free', 'plain', 'vanilla', 'electronic', 'texts', 'etexts', 'readable', 'by', 'both', 'humans', 'and', 'by', 'computers', 'since', '1971', 'these', 'etexts', 'prepared', 'by', 'hundreds', 'of', 'volunteers', 'and', 'donations', 'information', 'on', 'contacting', 'project', 'gutenberg', 'to', 'get', 'etexts', 'and', 'further', 'information', 'is', 'included', 'below', 'we', 'need', 'your', 'donations', 'alexanders', 'bridge', 'by', 'willa', 'cather', 'cather', '3', 'december', '1993', 'etext', '94', 'the', 'project', 'gutenberg', 'etext', 'of', 'alexanders', 'bridge', 'this', 'file', 'should', 'be', 'named', 'alexb10txt', 'or', 'alexb10zip', 'corrected', 'editions', 'of', 'our', 'etexts', 'get', 'a', 'new', 'number', 'alexb11txt', 'versions', 'based', 'on', 'separate', 'sources', 'get', 'new', 'letter', 'alexb10atxt', 'the', 'official', 'release', 'date', 'of', 'all', 'project', 'gutenberg', 'etexts', 'is', 'at', 'midnight', 'central', 'time', 'of', 'the', 'last', 'day', 'of', 'the', 'stated', 'month', 'a', 'preliminary', 'version', 'may', 'often', 'be', 'posted', 'for', 'suggestion', 'comment', 'and', 'editing', 'by', 'those', 'who', 'wish', 'to', 'do', 'so', 'to', 'be', 'sure', 'you', 'have', 'an', 'up', 'to', 'date', 'first', 'edition', 'xxxxx10xxxx', 'please', 'check', 'file', 'sizes', 'in', 'the', 'first', 'week', 'of', 'the', 'next', 'month', 'since', 'our', 'ftp', 'program', 'has', 'a', 'bug', 'in']\n",
      "Total Tokens: 30302\n",
      "Unique Tokens: 4300\n",
      "Total Sequences: 30251\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def clean_doc(doc):\n",
    "   \n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load document\n",
    "in_filename = 'alex.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "##making sentences with tokens\n",
    "length = 50+1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'o.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miwQn09YwTTJ"
   },
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'o.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)                  ##words\n",
    "sequences = tokenizer.texts_to_sequences(lines)               ##lines\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1563683644076,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "MiK4W3HXj4-S",
    "outputId": "d8fa4574-55f6-4308-ce0b-c78a162fe9dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1563683645712,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "NUlH_O61wTTP",
    "outputId": "8f428219-1614-48b5-e52a-4e587cca24b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4301"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APrLh2RIbGh1"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               41200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 51,401\n",
      "Trainable params: 51,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, input_shape=(31499, 2)))\n",
    "\n",
    "\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1276,
     "status": "ok",
     "timestamp": 1563952458126,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "fJVc58c8wTTb",
    "outputId": "c78f988b-fe0a-4f9b-86bf-5f11d8d9a0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Srinivas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            215050    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4301)              434401    \n",
      "=================================================================\n",
      "Total params: 800,351\n",
      "Trainable params: 800,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4703090,
     "status": "error",
     "timestamp": 1563870127217,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "M4TBa4-QNjpE",
    "outputId": "04893665-499d-41b6-e54f-3429dd98bacf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Srinivas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.4049 - acc: 0.6539\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.40487, saving model to jo.h5\n",
      "Epoch 2/200\n",
      "30251/30251 [==============================] - 71s 2ms/step - loss: 1.3115 - acc: 0.6838\n",
      "\n",
      "Epoch 00002: loss improved from 1.40487 to 1.31146, saving model to jo.h5\n",
      "Epoch 3/200\n",
      "30251/30251 [==============================] - 70s 2ms/step - loss: 1.2887 - acc: 0.6889\n",
      "\n",
      "Epoch 00003: loss improved from 1.31146 to 1.28870, saving model to jo.h5\n",
      "Epoch 4/200\n",
      "30251/30251 [==============================] - 71s 2ms/step - loss: 1.2881 - acc: 0.6889\n",
      "\n",
      "Epoch 00004: loss improved from 1.28870 to 1.28806, saving model to jo.h5\n",
      "Epoch 5/200\n",
      "30251/30251 [==============================] - 71s 2ms/step - loss: 1.2725 - acc: 0.6925\n",
      "\n",
      "Epoch 00005: loss improved from 1.28806 to 1.27250, saving model to jo.h5\n",
      "Epoch 6/200\n",
      "30251/30251 [==============================] - 71s 2ms/step - loss: 1.2557 - acc: 0.6975\n",
      "\n",
      "Epoch 00006: loss improved from 1.27250 to 1.25572, saving model to jo.h5\n",
      "Epoch 7/200\n",
      "30251/30251 [==============================] - 73s 2ms/step - loss: 1.2534 - acc: 0.6954\n",
      "\n",
      "Epoch 00007: loss improved from 1.25572 to 1.25341, saving model to jo.h5\n",
      "Epoch 8/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.2426 - acc: 0.6977\n",
      "\n",
      "Epoch 00008: loss improved from 1.25341 to 1.24261, saving model to jo.h5\n",
      "Epoch 9/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.2320 - acc: 0.7024\n",
      "\n",
      "Epoch 00009: loss improved from 1.24261 to 1.23198, saving model to jo.h5\n",
      "Epoch 10/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.2136 - acc: 0.7061\n",
      "\n",
      "Epoch 00010: loss improved from 1.23198 to 1.21356, saving model to jo.h5\n",
      "Epoch 11/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.2014 - acc: 0.7086\n",
      "\n",
      "Epoch 00011: loss improved from 1.21356 to 1.20137, saving model to jo.h5\n",
      "Epoch 12/200\n",
      "30251/30251 [==============================] - 75s 2ms/step - loss: 1.2009 - acc: 0.7093\n",
      "\n",
      "Epoch 00012: loss improved from 1.20137 to 1.20094, saving model to jo.h5\n",
      "Epoch 13/200\n",
      "30251/30251 [==============================] - 76s 3ms/step - loss: 1.2005 - acc: 0.7076\n",
      "\n",
      "Epoch 00013: loss improved from 1.20094 to 1.20048, saving model to jo.h5\n",
      "Epoch 14/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.1815 - acc: 0.7145\n",
      "\n",
      "Epoch 00014: loss improved from 1.20048 to 1.18145, saving model to jo.h5\n",
      "Epoch 15/200\n",
      "30251/30251 [==============================] - 76s 3ms/step - loss: 1.1708 - acc: 0.7175\n",
      "\n",
      "Epoch 00015: loss improved from 1.18145 to 1.17078, saving model to jo.h5\n",
      "Epoch 16/200\n",
      "30251/30251 [==============================] - 69s 2ms/step - loss: 1.1539 - acc: 0.7210\n",
      "\n",
      "Epoch 00016: loss improved from 1.17078 to 1.15393, saving model to jo.h5\n",
      "Epoch 17/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 1.1504 - acc: 0.7208\n",
      "\n",
      "Epoch 00017: loss improved from 1.15393 to 1.15039, saving model to jo.h5\n",
      "Epoch 18/200\n",
      "30251/30251 [==============================] - 70s 2ms/step - loss: 1.1422 - acc: 0.7237\n",
      "\n",
      "Epoch 00018: loss improved from 1.15039 to 1.14224, saving model to jo.h5\n",
      "Epoch 19/200\n",
      "30251/30251 [==============================] - 70s 2ms/step - loss: 1.1320 - acc: 0.7250\n",
      "\n",
      "Epoch 00019: loss improved from 1.14224 to 1.13200, saving model to jo.h5\n",
      "Epoch 20/200\n",
      "30251/30251 [==============================] - 69s 2ms/step - loss: 1.1223 - acc: 0.7298\n",
      "\n",
      "Epoch 00020: loss improved from 1.13200 to 1.12231, saving model to jo.h5\n",
      "Epoch 21/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 1.1115 - acc: 0.7297\n",
      "\n",
      "Epoch 00021: loss improved from 1.12231 to 1.11150, saving model to jo.h5\n",
      "Epoch 22/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 1.1157 - acc: 0.7282\n",
      "\n",
      "Epoch 00022: loss did not improve from 1.11150\n",
      "Epoch 23/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 1.1113 - acc: 0.7289\n",
      "\n",
      "Epoch 00023: loss improved from 1.11150 to 1.11132, saving model to jo.h5\n",
      "Epoch 24/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 1.0831 - acc: 0.7400\n",
      "\n",
      "Epoch 00024: loss improved from 1.11132 to 1.08311, saving model to jo.h5\n",
      "Epoch 25/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 1.0708 - acc: 0.7398\n",
      "\n",
      "Epoch 00025: loss improved from 1.08311 to 1.07078, saving model to jo.h5\n",
      "Epoch 26/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 1.0626 - acc: 0.7420\n",
      "\n",
      "Epoch 00026: loss improved from 1.07078 to 1.06260, saving model to jo.h5\n",
      "Epoch 27/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 1.0611 - acc: 0.7423\n",
      "\n",
      "Epoch 00027: loss improved from 1.06260 to 1.06114, saving model to jo.h5\n",
      "Epoch 28/200\n",
      "30251/30251 [==============================] - 70s 2ms/step - loss: 1.0482 - acc: 0.7441\n",
      "\n",
      "Epoch 00028: loss improved from 1.06114 to 1.04816, saving model to jo.h5\n",
      "Epoch 29/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 1.0383 - acc: 0.7468\n",
      "\n",
      "Epoch 00029: loss improved from 1.04816 to 1.03825, saving model to jo.h5\n",
      "Epoch 30/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 1.0428 - acc: 0.7457\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.03825\n",
      "Epoch 31/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 1.0366 - acc: 0.7475\n",
      "\n",
      "Epoch 00031: loss improved from 1.03825 to 1.03662, saving model to jo.h5\n",
      "Epoch 32/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 1.0154 - acc: 0.7528\n",
      "\n",
      "Epoch 00032: loss improved from 1.03662 to 1.01543, saving model to jo.h5\n",
      "Epoch 33/200\n",
      "30251/30251 [==============================] - 71s 2ms/step - loss: 0.9941 - acc: 0.7609\n",
      "\n",
      "Epoch 00033: loss improved from 1.01543 to 0.99409, saving model to jo.h5\n",
      "Epoch 34/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.9838 - acc: 0.7608\n",
      "\n",
      "Epoch 00034: loss improved from 0.99409 to 0.98383, saving model to jo.h5\n",
      "Epoch 35/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.9989 - acc: 0.7569\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.98383\n",
      "Epoch 36/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.9988 - acc: 0.7556\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.98383\n",
      "Epoch 37/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.9788 - acc: 0.7624\n",
      "\n",
      "Epoch 00037: loss improved from 0.98383 to 0.97880, saving model to jo.h5\n",
      "Epoch 38/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.9628 - acc: 0.7673\n",
      "\n",
      "Epoch 00038: loss improved from 0.97880 to 0.96276, saving model to jo.h5\n",
      "Epoch 39/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.9463 - acc: 0.7710\n",
      "\n",
      "Epoch 00039: loss improved from 0.96276 to 0.94630, saving model to jo.h5\n",
      "Epoch 40/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.9521 - acc: 0.7691\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.94630\n",
      "Epoch 41/200\n",
      "30251/30251 [==============================] - 70s 2ms/step - loss: 0.9583 - acc: 0.7639\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.94630\n",
      "Epoch 42/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.9416 - acc: 0.7705\n",
      "\n",
      "Epoch 00042: loss improved from 0.94630 to 0.94163, saving model to jo.h5\n",
      "Epoch 43/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.9257 - acc: 0.7748\n",
      "\n",
      "Epoch 00043: loss improved from 0.94163 to 0.92573, saving model to jo.h5\n",
      "Epoch 44/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.9072 - acc: 0.7821\n",
      "\n",
      "Epoch 00044: loss improved from 0.92573 to 0.90723, saving model to jo.h5\n",
      "Epoch 45/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.8930 - acc: 0.7837\n",
      "\n",
      "Epoch 00045: loss improved from 0.90723 to 0.89302, saving model to jo.h5\n",
      "Epoch 46/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.8948 - acc: 0.7847\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.89302\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8913 - acc: 0.7839\n",
      "\n",
      "Epoch 00047: loss improved from 0.89302 to 0.89133, saving model to jo.h5\n",
      "Epoch 48/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.8890 - acc: 0.7839\n",
      "\n",
      "Epoch 00048: loss improved from 0.89133 to 0.88896, saving model to jo.h5\n",
      "Epoch 49/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.8764 - acc: 0.7877\n",
      "\n",
      "Epoch 00049: loss improved from 0.88896 to 0.87642, saving model to jo.h5\n",
      "Epoch 50/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.8731 - acc: 0.7885\n",
      "\n",
      "Epoch 00050: loss improved from 0.87642 to 0.87311, saving model to jo.h5\n",
      "Epoch 51/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8712 - acc: 0.7888\n",
      "\n",
      "Epoch 00051: loss improved from 0.87311 to 0.87119, saving model to jo.h5\n",
      "Epoch 52/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8697 - acc: 0.7874\n",
      "\n",
      "Epoch 00052: loss improved from 0.87119 to 0.86965, saving model to jo.h5\n",
      "Epoch 53/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8555 - acc: 0.7922\n",
      "\n",
      "Epoch 00053: loss improved from 0.86965 to 0.85555, saving model to jo.h5\n",
      "Epoch 54/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8367 - acc: 0.7968\n",
      "\n",
      "Epoch 00054: loss improved from 0.85555 to 0.83671, saving model to jo.h5\n",
      "Epoch 55/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.8274 - acc: 0.7999\n",
      "\n",
      "Epoch 00055: loss improved from 0.83671 to 0.82735, saving model to jo.h5\n",
      "Epoch 56/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8181 - acc: 0.8019\n",
      "\n",
      "Epoch 00056: loss improved from 0.82735 to 0.81812, saving model to jo.h5\n",
      "Epoch 57/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.8113 - acc: 0.8033\n",
      "\n",
      "Epoch 00057: loss improved from 0.81812 to 0.81128, saving model to jo.h5\n",
      "Epoch 58/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8014 - acc: 0.8079\n",
      "\n",
      "Epoch 00058: loss improved from 0.81128 to 0.80142, saving model to jo.h5\n",
      "Epoch 59/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8184 - acc: 0.7998\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.80142\n",
      "Epoch 60/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.8060 - acc: 0.8071\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.80142\n",
      "Epoch 61/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.7910 - acc: 0.8081\n",
      "\n",
      "Epoch 00061: loss improved from 0.80142 to 0.79098, saving model to jo.h5\n",
      "Epoch 62/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.8064 - acc: 0.8021\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.79098\n",
      "Epoch 63/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.7835 - acc: 0.8103\n",
      "\n",
      "Epoch 00063: loss improved from 0.79098 to 0.78346, saving model to jo.h5\n",
      "Epoch 64/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.7823 - acc: 0.8075\n",
      "\n",
      "Epoch 00064: loss improved from 0.78346 to 0.78227, saving model to jo.h5\n",
      "Epoch 65/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.7623 - acc: 0.8156\n",
      "\n",
      "Epoch 00065: loss improved from 0.78227 to 0.76230, saving model to jo.h5\n",
      "Epoch 66/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.7412 - acc: 0.8229\n",
      "\n",
      "Epoch 00066: loss improved from 0.76230 to 0.74115, saving model to jo.h5\n",
      "Epoch 67/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.7417 - acc: 0.8216\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.74115\n",
      "Epoch 68/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.7225 - acc: 0.8274\n",
      "\n",
      "Epoch 00068: loss improved from 0.74115 to 0.72254, saving model to jo.h5\n",
      "Epoch 69/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.7367 - acc: 0.8229\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.72254\n",
      "Epoch 70/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.7538 - acc: 0.8159\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.72254\n",
      "Epoch 71/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.7254 - acc: 0.8253\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.72254\n",
      "Epoch 72/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.7105 - acc: 0.8285\n",
      "\n",
      "Epoch 00072: loss improved from 0.72254 to 0.71053, saving model to jo.h5\n",
      "Epoch 73/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.7183 - acc: 0.8257\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.71053\n",
      "Epoch 74/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.6990 - acc: 0.8326\n",
      "\n",
      "Epoch 00074: loss improved from 0.71053 to 0.69895, saving model to jo.h5\n",
      "Epoch 75/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6874 - acc: 0.8359\n",
      "\n",
      "Epoch 00075: loss improved from 0.69895 to 0.68739, saving model to jo.h5\n",
      "Epoch 76/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6928 - acc: 0.8332\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.68739\n",
      "Epoch 77/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6852 - acc: 0.8347\n",
      "\n",
      "Epoch 00077: loss improved from 0.68739 to 0.68523, saving model to jo.h5\n",
      "Epoch 78/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6636 - acc: 0.8407\n",
      "\n",
      "Epoch 00078: loss improved from 0.68523 to 0.66360, saving model to jo.h5\n",
      "Epoch 79/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6553 - acc: 0.8439\n",
      "\n",
      "Epoch 00079: loss improved from 0.66360 to 0.65534, saving model to jo.h5\n",
      "Epoch 80/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.6680 - acc: 0.8379\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.65534\n",
      "Epoch 81/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.6789 - acc: 0.8341\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.65534\n",
      "Epoch 82/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6625 - acc: 0.8399\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.65534\n",
      "Epoch 83/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6311 - acc: 0.8491\n",
      "\n",
      "Epoch 00083: loss improved from 0.65534 to 0.63107, saving model to jo.h5\n",
      "Epoch 84/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.6235 - acc: 0.8518\n",
      "\n",
      "Epoch 00084: loss improved from 0.63107 to 0.62348, saving model to jo.h5\n",
      "Epoch 85/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6299 - acc: 0.8501\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.62348\n",
      "Epoch 86/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6367 - acc: 0.8450\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.62348\n",
      "Epoch 87/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.6392 - acc: 0.8436\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.62348\n",
      "Epoch 88/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.6261 - acc: 0.8495\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.62348\n",
      "Epoch 89/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 0.6181 - acc: 0.8509\n",
      "\n",
      "Epoch 00089: loss improved from 0.62348 to 0.61813, saving model to jo.h5\n",
      "Epoch 90/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.5929 - acc: 0.8586\n",
      "\n",
      "Epoch 00090: loss improved from 0.61813 to 0.59287, saving model to jo.h5\n",
      "Epoch 91/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5927 - acc: 0.8587\n",
      "\n",
      "Epoch 00091: loss improved from 0.59287 to 0.59273, saving model to jo.h5\n",
      "Epoch 92/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5798 - acc: 0.8638\n",
      "\n",
      "Epoch 00092: loss improved from 0.59273 to 0.57980, saving model to jo.h5\n",
      "Epoch 93/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5886 - acc: 0.8594\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.57980\n",
      "Epoch 94/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5867 - acc: 0.8601\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.57980\n",
      "Epoch 95/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5971 - acc: 0.8552\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.57980\n",
      "Epoch 96/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5978 - acc: 0.8555\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.57980\n",
      "Epoch 97/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5776 - acc: 0.8628\n",
      "\n",
      "Epoch 00097: loss improved from 0.57980 to 0.57761, saving model to jo.h5\n",
      "Epoch 98/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5718 - acc: 0.8632\n",
      "\n",
      "Epoch 00098: loss improved from 0.57761 to 0.57182, saving model to jo.h5\n",
      "Epoch 99/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.5557 - acc: 0.8676\n",
      "\n",
      "Epoch 00099: loss improved from 0.57182 to 0.55574, saving model to jo.h5\n",
      "Epoch 100/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5345 - acc: 0.8749\n",
      "\n",
      "Epoch 00100: loss improved from 0.55574 to 0.53450, saving model to jo.h5\n",
      "Epoch 101/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5300 - acc: 0.8763\n",
      "\n",
      "Epoch 00101: loss improved from 0.53450 to 0.52999, saving model to jo.h5\n",
      "Epoch 102/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5433 - acc: 0.8703\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.52999\n",
      "Epoch 103/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5383 - acc: 0.8721\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.52999\n",
      "Epoch 104/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5339 - acc: 0.8717\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.52999\n",
      "Epoch 105/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5391 - acc: 0.8704\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.52999\n",
      "Epoch 106/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5177 - acc: 0.8780\n",
      "\n",
      "Epoch 00106: loss improved from 0.52999 to 0.51770, saving model to jo.h5\n",
      "Epoch 107/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5068 - acc: 0.8810\n",
      "\n",
      "Epoch 00107: loss improved from 0.51770 to 0.50679, saving model to jo.h5\n",
      "Epoch 108/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5035 - acc: 0.8837\n",
      "\n",
      "Epoch 00108: loss improved from 0.50679 to 0.50345, saving model to jo.h5\n",
      "Epoch 109/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5074 - acc: 0.8803\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.50345\n",
      "Epoch 110/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.5284 - acc: 0.8717\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.50345\n",
      "Epoch 111/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5039 - acc: 0.8808\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.50345\n",
      "Epoch 112/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4948 - acc: 0.8823\n",
      "\n",
      "Epoch 00112: loss improved from 0.50345 to 0.49482, saving model to jo.h5\n",
      "Epoch 113/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.5013 - acc: 0.8793\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.49482\n",
      "Epoch 114/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4775 - acc: 0.8879\n",
      "\n",
      "Epoch 00114: loss improved from 0.49482 to 0.47746, saving model to jo.h5\n",
      "Epoch 115/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4615 - acc: 0.8936\n",
      "\n",
      "Epoch 00115: loss improved from 0.47746 to 0.46149, saving model to jo.h5\n",
      "Epoch 116/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4542 - acc: 0.8961\n",
      "\n",
      "Epoch 00116: loss improved from 0.46149 to 0.45422, saving model to jo.h5\n",
      "Epoch 117/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4423 - acc: 0.8978\n",
      "\n",
      "Epoch 00117: loss improved from 0.45422 to 0.44233, saving model to jo.h5\n",
      "Epoch 118/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4530 - acc: 0.8944\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.44233\n",
      "Epoch 119/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4799 - acc: 0.8853\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.44233\n",
      "Epoch 120/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.4896 - acc: 0.8793\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.44233\n",
      "Epoch 121/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4728 - acc: 0.8875\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.44233\n",
      "Epoch 122/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4651 - acc: 0.8881\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.44233\n",
      "Epoch 123/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4801 - acc: 0.8838\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.44233\n",
      "Epoch 124/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4404 - acc: 0.8965\n",
      "\n",
      "Epoch 00124: loss improved from 0.44233 to 0.44037, saving model to jo.h5\n",
      "Epoch 125/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4275 - acc: 0.9009\n",
      "\n",
      "Epoch 00125: loss improved from 0.44037 to 0.42746, saving model to jo.h5\n",
      "Epoch 126/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4201 - acc: 0.9039\n",
      "\n",
      "Epoch 00126: loss improved from 0.42746 to 0.42007, saving model to jo.h5\n",
      "Epoch 127/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4050 - acc: 0.9073\n",
      "\n",
      "Epoch 00127: loss improved from 0.42007 to 0.40501, saving model to jo.h5\n",
      "Epoch 128/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.4015 - acc: 0.9103\n",
      "\n",
      "Epoch 00128: loss improved from 0.40501 to 0.40155, saving model to jo.h5\n",
      "Epoch 129/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.4086 - acc: 0.9056\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.40155\n",
      "Epoch 130/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.4293 - acc: 0.8972\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.40155\n",
      "Epoch 131/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.4327 - acc: 0.8966\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.40155\n",
      "Epoch 132/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.4462 - acc: 0.8914\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.40155\n",
      "Epoch 133/200\n",
      "30251/30251 [==============================] - 258s 9ms/step - loss: 0.4604 - acc: 0.8840\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.40155\n",
      "Epoch 134/200\n",
      "30251/30251 [==============================] - 72s 2ms/step - loss: 0.4386 - acc: 0.8936\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.40155\n",
      "Epoch 135/200\n",
      "30251/30251 [==============================] - 76s 3ms/step - loss: 0.3890 - acc: 0.9100\n",
      "\n",
      "Epoch 00135: loss improved from 0.40155 to 0.38900, saving model to jo.h5\n",
      "Epoch 136/200\n",
      "30251/30251 [==============================] - 74s 2ms/step - loss: 0.3656 - acc: 0.9186\n",
      "\n",
      "Epoch 00136: loss improved from 0.38900 to 0.36555, saving model to jo.h5\n",
      "Epoch 137/200\n",
      "30251/30251 [==============================] - 74s 2ms/step - loss: 0.3381 - acc: 0.9286\n",
      "\n",
      "Epoch 00137: loss improved from 0.36555 to 0.33806, saving model to jo.h5\n",
      "Epoch 138/200\n",
      "30251/30251 [==============================] - 69s 2ms/step - loss: 0.3378 - acc: 0.9283\n",
      "\n",
      "Epoch 00138: loss improved from 0.33806 to 0.33782, saving model to jo.h5\n",
      "Epoch 139/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.3520 - acc: 0.9218\n",
      "\n",
      "Epoch 00139: loss did not improve from 0.33782\n",
      "Epoch 140/200\n",
      "30251/30251 [==============================] - 69s 2ms/step - loss: 0.4150 - acc: 0.8981\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.33782\n",
      "Epoch 141/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.4542 - acc: 0.8853\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.33782\n",
      "Epoch 142/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.4285 - acc: 0.8921\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.33782\n",
      "Epoch 143/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.3845 - acc: 0.9086\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.33782\n",
      "Epoch 144/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.3743 - acc: 0.9115\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.33782\n",
      "Epoch 145/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3437 - acc: 0.9223\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.33782\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3349 - acc: 0.9257\n",
      "\n",
      "Epoch 00146: loss improved from 0.33782 to 0.33490, saving model to jo.h5\n",
      "Epoch 147/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3169 - acc: 0.9329\n",
      "\n",
      "Epoch 00147: loss improved from 0.33490 to 0.31690, saving model to jo.h5\n",
      "Epoch 148/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.3353 - acc: 0.9259\n",
      "\n",
      "Epoch 00148: loss did not improve from 0.31690\n",
      "Epoch 149/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.3596 - acc: 0.9154\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.31690\n",
      "Epoch 150/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.3546 - acc: 0.9174\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.31690\n",
      "Epoch 151/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3756 - acc: 0.9086\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.31690\n",
      "Epoch 152/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.3684 - acc: 0.9107\n",
      "\n",
      "Epoch 00152: loss did not improve from 0.31690\n",
      "Epoch 153/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3614 - acc: 0.9117\n",
      "\n",
      "Epoch 00153: loss did not improve from 0.31690\n",
      "Epoch 154/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.3545 - acc: 0.9160\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.31690\n",
      "Epoch 155/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.3009 - acc: 0.9344\n",
      "\n",
      "Epoch 00155: loss improved from 0.31690 to 0.30085, saving model to jo.h5\n",
      "Epoch 156/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3091 - acc: 0.9312\n",
      "\n",
      "Epoch 00156: loss did not improve from 0.30085\n",
      "Epoch 157/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3225 - acc: 0.9261\n",
      "\n",
      "Epoch 00157: loss did not improve from 0.30085\n",
      "Epoch 158/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3164 - acc: 0.9277\n",
      "\n",
      "Epoch 00158: loss did not improve from 0.30085\n",
      "Epoch 159/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3102 - acc: 0.9307\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.30085\n",
      "Epoch 160/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3234 - acc: 0.9242\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.30085\n",
      "Epoch 161/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3473 - acc: 0.9170\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.30085\n",
      "Epoch 162/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3769 - acc: 0.9062\n",
      "\n",
      "Epoch 00162: loss did not improve from 0.30085\n",
      "Epoch 163/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3260 - acc: 0.9238\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.30085\n",
      "Epoch 164/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2892 - acc: 0.9367\n",
      "\n",
      "Epoch 00164: loss improved from 0.30085 to 0.28918, saving model to jo.h5\n",
      "Epoch 165/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2618 - acc: 0.9451\n",
      "\n",
      "Epoch 00165: loss improved from 0.28918 to 0.26185, saving model to jo.h5\n",
      "Epoch 166/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2547 - acc: 0.9473\n",
      "\n",
      "Epoch 00166: loss improved from 0.26185 to 0.25467, saving model to jo.h5\n",
      "Epoch 167/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.2536 - acc: 0.9474\n",
      "\n",
      "Epoch 00167: loss improved from 0.25467 to 0.25362, saving model to jo.h5\n",
      "Epoch 168/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2634 - acc: 0.9445\n",
      "\n",
      "Epoch 00168: loss did not improve from 0.25362\n",
      "Epoch 169/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.3186 - acc: 0.9241\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.25362\n",
      "Epoch 170/200\n",
      "30251/30251 [==============================] - 68s 2ms/step - loss: 0.3781 - acc: 0.9022\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.25362\n",
      "Epoch 171/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3820 - acc: 0.9025\n",
      "\n",
      "Epoch 00171: loss did not improve from 0.25362\n",
      "Epoch 172/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.3728 - acc: 0.9059\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.25362\n",
      "Epoch 173/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.2964 - acc: 0.9324\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.25362\n",
      "Epoch 174/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2462 - acc: 0.9493\n",
      "\n",
      "Epoch 00174: loss improved from 0.25362 to 0.24618, saving model to jo.h5\n",
      "Epoch 175/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2284 - acc: 0.9555\n",
      "\n",
      "Epoch 00175: loss improved from 0.24618 to 0.22836, saving model to jo.h5\n",
      "Epoch 176/200\n",
      "30251/30251 [==============================] - 67s 2ms/step - loss: 0.2290 - acc: 0.9546\n",
      "\n",
      "Epoch 00176: loss did not improve from 0.22836\n",
      "Epoch 177/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2301 - acc: 0.9525\n",
      "\n",
      "Epoch 00177: loss did not improve from 0.22836\n",
      "Epoch 178/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2404 - acc: 0.9492\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.22836\n",
      "Epoch 179/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.2589 - acc: 0.9422\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.22836\n",
      "Epoch 180/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2920 - acc: 0.9295\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.22836\n",
      "Epoch 181/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.3434 - acc: 0.9130\n",
      "\n",
      "Epoch 00181: loss did not improve from 0.22836\n",
      "Epoch 182/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3468 - acc: 0.9111\n",
      "\n",
      "Epoch 00182: loss did not improve from 0.22836\n",
      "Epoch 183/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3211 - acc: 0.9204\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.22836\n",
      "Epoch 184/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2671 - acc: 0.9381\n",
      "\n",
      "Epoch 00184: loss did not improve from 0.22836\n",
      "Epoch 185/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2318 - acc: 0.9507\n",
      "\n",
      "Epoch 00185: loss did not improve from 0.22836\n",
      "Epoch 186/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.2101 - acc: 0.9577\n",
      "\n",
      "Epoch 00186: loss improved from 0.22836 to 0.21011, saving model to jo.h5\n",
      "Epoch 187/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.1903 - acc: 0.9658\n",
      "\n",
      "Epoch 00187: loss improved from 0.21011 to 0.19031, saving model to jo.h5\n",
      "Epoch 188/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.1868 - acc: 0.9645\n",
      "\n",
      "Epoch 00188: loss improved from 0.19031 to 0.18678, saving model to jo.h5\n",
      "Epoch 189/200\n",
      "30251/30251 [==============================] - 66s 2ms/step - loss: 0.2270 - acc: 0.9512\n",
      "\n",
      "Epoch 00189: loss did not improve from 0.18678\n",
      "Epoch 190/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2749 - acc: 0.9335\n",
      "\n",
      "Epoch 00190: loss did not improve from 0.18678\n",
      "Epoch 191/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3447 - acc: 0.9098\n",
      "\n",
      "Epoch 00191: loss did not improve from 0.18678\n",
      "Epoch 192/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.3544 - acc: 0.9053\n",
      "\n",
      "Epoch 00192: loss did not improve from 0.18678\n",
      "Epoch 193/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2938 - acc: 0.9267\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.18678\n",
      "Epoch 194/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.2309 - acc: 0.9499\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.18678\n",
      "Epoch 195/200\n",
      "30251/30251 [==============================] - 65s 2ms/step - loss: 0.2015 - acc: 0.9600\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.18678\n",
      "Epoch 196/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.1981 - acc: 0.9599\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.18678\n",
      "Epoch 197/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.1899 - acc: 0.9626\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.18678\n",
      "Epoch 198/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.1842 - acc: 0.9644\n",
      "\n",
      "Epoch 00198: loss improved from 0.18678 to 0.18415, saving model to jo.h5\n",
      "Epoch 199/200\n",
      "30251/30251 [==============================] - 64s 2ms/step - loss: 0.1804 - acc: 0.9665\n",
      "\n",
      "Epoch 00199: loss improved from 0.18415 to 0.18043, saving model to jo.h5\n",
      "Epoch 200/200\n",
      "30251/30251 [==============================] - 63s 2ms/step - loss: 0.2596 - acc: 0.9394\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.18043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f23124278>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "file=\"jo.h5\"\n",
    "checkpoint = ModelCheckpoint(file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y,epochs=200,batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RLkSdIj0Iy-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoCfBKcIdzir"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.load_weights('jo.h5')\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORl_WFAMLBpn"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "#model.save('/content/drive/My Drive/Colab Notebooks/text/models/model.h5')\n",
    "# save the tokenizer\n",
    "#dump(tokenizer, open('/content/drive/My Drive/Colab Notebooks/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihu_5CkJwTTm"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArGirnrt5srP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_seq(model,seq_length, input_text, n_words):\n",
    "    result = list()\n",
    "    in_text = input_text\n",
    "    #generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "      # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        out = model.predict_classes(encoded, verbose=0)\n",
    "        # mapping predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == out:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1563952477840,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "dsXLEimi_Z9k",
    "outputId": "844bef93-02e8-4701-f9fd-5da6661d872a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her hand is it you indeed professor wilson i was afraid that you might get here before i did i was detained at a concert and bartley telephoned that he would be late thomas will show you your room had you rather have your tea brought to you there or will\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1474,
     "status": "ok",
     "timestamp": 1563952547410,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "tHQH-uASAA4W",
    "outputId": "987b0995-a096-407d-f136-0cdc53fb8a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have it down here with me while we wait\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model,seq_length, input_text,10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1639,
     "status": "ok",
     "timestamp": 1563952620090,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "_GKZJX8YBG2y",
    "outputId": "ebc841a3-ed0a-4700-d023-838682f7b3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stated that the engineer himself was in town and at his office on west tenth street on sunday the day after this notice appeared alexander worked all day at his tenth street rooms his business often called him to new york and he had kept an apartment there for years subletting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1585,
     "status": "ok",
     "timestamp": 1563952621466,
     "user": {
      "displayName": "joshna rani pothuganti",
      "photoUrl": "https://lh4.googleusercontent.com/-EPdOBtVslhI/AAAAAAAAAAI/AAAAAAAAAC8/RbQ-cPBAoDg/s64/photo.jpg",
      "userId": "15796857545654297369"
     },
     "user_tz": -330
    },
    "id": "AW8v7SjxBJFs",
    "outputId": "6653b663-0a92-4360-a184-034e929ca088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it when he went abroad for any length of time\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model,seq_length, input_text,10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzIPzn4foakg"
   },
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1563870286700,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "0MFtU6-17euf",
    "outputId": "2fc29cb4-29c5-422c-ae79-8d2fb33cd873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the grateful moisture but the complete peace of the first part of the voyage was over sometimes he rose suddenly from his chair as if driven out and paced the deck for hours people noticed his propensity for walking in rough weather and watched him curiously as he did his rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1395,
     "status": "ok",
     "timestamp": 1563870289046,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "7anid_1l7nMV",
    "outputId": "de0734bd-4b69-477c-a477-51b2bcb36ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from his abstraction and the determined set of his jaw\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model,seq_length, input_text,10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1563887832668,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "m4UjByJE7kp5",
    "outputId": "9bae07ec-2394-415b-97b9-7d0f73753e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his experience it was more precious than honors or achievement in all those busy successful years there had been nothing so good as this hour of wild lightheartedness this feeling was the only happiness that was real to him and such hours were the only ones in which he could feel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1563887838335,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "5otk0boH7rQU",
    "outputId": "eab4b1d8-2653-43cb-8882-ca663fa143e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his own continuous identity feel the boy he had been\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model,seq_length, input_text,10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1563870294666,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "yqTv74Wy7kN6",
    "outputId": "82b62cb3-8c9c-4437-ef57-ea5db5152aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air he knew he should live to tell her and to recover all he had lost now at last he felt sure of himself he was not startled it seemed to him that he had been through something of this sort before there was nothing horrible about it this too was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "input_text = lines[random.randint(0,len(lines))]\n",
    "print(input_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1213,
     "status": "ok",
     "timestamp": 1563870297167,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "jRlDKnYQ7eof",
    "outputId": "7ddc2e03-21f5-453a-d0cb-39cdf43f1e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life and life was activity just as it was in\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model,seq_length, input_text,10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIRrlB3m8fFm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL_0Ld9f8fAs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzSdrtkn0YSz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen(input_text):\n",
    "    \n",
    "    generated = generate_seq(model,seq_length, input_text,200)\n",
    "    print('output text \\n')\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7709,
     "status": "ok",
     "timestamp": 1563636806372,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "oe36zzfg0pOJ",
    "outputId": "0bef72a7-cac4-422e-b60b-c23e4a117277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter input text \n",
      "given it a glory and a part to play again in the symphony\n",
      "output text \n",
      "\n",
      "that rules the day and the night and now its marching onward through the realms of old romance and trolling out a fond familiar tune and now its roaring cannon down to fight the king of france and now its prattling softly to the moon and all around the organ theres a sea without a thin soft his rugs over the morning with the fountain and smelling the spice of the sycamores in dance a portraitpainter of dancing another michael entrances and were hats and the train without suggest new comedy little hilda may really noticing that you mrs alexander smiled on on his head on his hands hanging between bedford square he stepped back into her into the hipswho ever heard it out of galway she saves her hand too shes at so many happy full was dull before dinner dissects the dull purpose and a part of him with his arm then i did not allow her the realest thing i have him over some two professional little great tea where more the collar of that there none of with a bluish angora liked the nobleness consciousness of a gale up alexander rose cleared smiled in his prime\n"
     ]
    }
   ],
   "source": [
    "gen(input('enter input text \\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hzWEprdsjyt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBgZCC84skaM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1563297924442,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "uswL9PTwknoD",
    "outputId": "3a26c41e-3ef9-45df-82e0-06c7874ac700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4807,
     "status": "ok",
     "timestamp": 1563276018458,
     "user": {
      "displayName": "joshna rani Pothuganti",
      "photoUrl": "https://lh3.googleusercontent.com/-Q1AiJLKssYM/AAAAAAAAAAI/AAAAAAAAAHk/E_fsC-xGjNA/s64/photo.jpg",
      "userId": "15974000085444700071"
     },
     "user_tz": -330
    },
    "id": "ka0j5EGC6dKm",
    "outputId": "a7a03a6d-6267-441a-c7b5-023a1d3f1a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter input text \n",
      "The sun sank rapidly; the silvery light had faded from the bare boughs\n",
      "----------------------------------------\n",
      "output text \n",
      "\n",
      "and the watery twilight was setting in as them in\n"
     ]
    }
   ],
   "source": [
    "##gen(input('enter input text \\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1j7flF9TtjBi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gener text.ipynb",
   "provenance": [
    {
     "file_id": "1uRBBEoFEOenQpPKZCUZI6Gyc8b-k2q_8",
     "timestamp": 1563625892204
    },
    {
     "file_id": "1rVwDQWy6CxkU90dPQIu2lOZWk8t-akXd",
     "timestamp": 1563556180652
    },
    {
     "file_id": "1-OH2oRErGiGtlPqTY43eZgUG0J_MvOiZ",
     "timestamp": 1563445774349
    },
    {
     "file_id": "1_RB-S5kgnCdCrT1u7eBGflzNvqNyCsrj",
     "timestamp": 1563212488324
    },
    {
     "file_id": "14vKtRSH96Iyq1jLx2vcMmYW4bkrIcT35",
     "timestamp": 1563176382056
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
